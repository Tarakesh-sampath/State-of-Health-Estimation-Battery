{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:34:02.407039: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-28 18:34:02.410335: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-28 18:34:02.419254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 18:34:02.433425: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 18:34:02.437565: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 18:34:02.449690: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 18:34:03.581573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#loading dat\n",
    "file_1=\"../Dataset/B0005_dis_dataset.csv\"\n",
    "file_2=\"../Dataset/B0006_dis_dataset.csv\"\n",
    "dataset_1=pd.read_csv(file_1)\n",
    "dataset_2=pd.read_csv(file_2)\n",
    "\n",
    "C_max=2.05\n",
    "\n",
    "#renaming columns for simplicity\n",
    "dataset_1.columns=['cycle',\"capacity\",'voltage', 'current','temperature', 'current_load', 'voltage_load',\"time\"]\n",
    "dataset_1[\"current_load\"]= abs(dataset_1[\"current_load\"])\n",
    "\n",
    "#removing ideling / starting and ending current level data  \n",
    "df_1=dataset_1[(dataset_1.current_load>=0.006)].reset_index(drop=True)\n",
    "\n",
    "# converting time seconds->hour\n",
    "df_1[\"time\"]=df_1[\"time\"]/3600\n",
    "\n",
    "#calculate Soc for each cycle\n",
    "cycle_count=df_1[\"cycle\"].unique()\n",
    "Soc_list=[]\n",
    "\n",
    "# Update Soc for each reading within each cycle using Coulomb counting\n",
    "for cycle in cycle_count:\n",
    "    cycle_data = df_1[df_1['cycle'] == cycle]\n",
    "    \n",
    "    cumulative_Soc = [sum(cycle_data[\"capacity\"])/len(cycle_data[\"capacity\"])]\n",
    "    \n",
    "    for i in range(1, len(cycle_data)):\n",
    "        \n",
    "        delta_time = cycle_data['time'].iloc[i-1] - cycle_data['time'].iloc[i]\n",
    "        \n",
    "        delta_charge = cycle_data['current'].iloc[i] * delta_time\n",
    "        \n",
    "        cumulative_Soc.append(cumulative_Soc[-1] -delta_charge)\n",
    "        \n",
    "    Soc_list.extend(cumulative_Soc)\n",
    "\n",
    "#DOD is the opposite of the soc \n",
    "df_1[\"Soc\"] = [(q / C_max) for q in Soc_list]\n",
    "df_1[\"Soh\"]=df_1[\"capacity\"]/C_max\n",
    "\n",
    "# selecting parameters \n",
    "att_=[\"cycle\",\"capacity\",\"voltage\",\"current\",\"temperature\",\"current_load\",\"voltage_load\",\"time\",\"Soc\"]\n",
    "att=[\"cycle\",\"current_load\",\"voltage_load\",\"Soc\",\"time\"]\n",
    "X_axis=\"cycle\"\n",
    "\n",
    "#seperating dataset\n",
    "X_train = df_1[att]\n",
    "y_train = df_1[[\"Soh\"]]\n",
    "\n",
    "\n",
    "\n",
    "#renaming columns for simplicity\n",
    "dataset_2.columns=['cycle',\"capacity\",'voltage', 'current','temperature', 'current_load', 'voltage_load',\"time\"]\n",
    "dataset_2[\"current_load\"]= abs(dataset_2[\"current_load\"])\n",
    "\n",
    "#removing ideling / starting and ending current level data  \n",
    "df_2=dataset_2[(dataset_2.current_load>=0.006)].reset_index(drop=True)\n",
    "\n",
    "# converting time seconds->hour\n",
    "df_2[\"time\"]=df_2[\"time\"]/3600\n",
    "\n",
    "#calculate Soc for each cycle\n",
    "cycle_count=df_2[\"cycle\"].unique()\n",
    "Soc_list=[]\n",
    "\n",
    "# Update Soc for each reading within each cycle using Coulomb counting\n",
    "for cycle in cycle_count:\n",
    "    cycle_data = df_2[df_2['cycle'] == cycle]\n",
    "    \n",
    "    cumulative_Soc = [sum(cycle_data[\"capacity\"])/len(cycle_data[\"capacity\"])]\n",
    "    \n",
    "    for i in range(1, len(cycle_data)):\n",
    "        \n",
    "        delta_time = cycle_data['time'].iloc[i-1] - cycle_data['time'].iloc[i]\n",
    "        \n",
    "        delta_charge = cycle_data['current'].iloc[i] * delta_time\n",
    "        \n",
    "        cumulative_Soc.append(cumulative_Soc[-1] -delta_charge)\n",
    "        \n",
    "    Soc_list.extend(cumulative_Soc)\n",
    "\n",
    "#DOD is the opposite of the soc \n",
    "df_2[\"Soc\"] = [(q / C_max) for q in Soc_list]\n",
    "df_2[\"Soh\"]=df_2[\"capacity\"]/C_max\n",
    "\n",
    "X_test = df_2[att]\n",
    "y_test = df_2[[\"Soh\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0984 - val_loss: 8.9892e-05\n",
      "Epoch 2/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.1605e-04 - val_loss: 9.4609e-05\n",
      "Epoch 3/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.8574e-05 - val_loss: 4.1119e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.6740e-05 - val_loss: 5.5591e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4894e-05 - val_loss: 2.3749e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3303e-05 - val_loss: 1.0816e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 6.0636e-06 - val_loss: 3.7699e-05\n",
      "Epoch 8/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.2091e-06 - val_loss: 5.1131e-05\n",
      "Epoch 9/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 4.0070e-06 - val_loss: 8.9570e-06\n",
      "Epoch 10/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.2454e-06 - val_loss: 4.9741e-06\n",
      "Epoch 11/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.0374e-06 - val_loss: 3.8902e-06\n",
      "Epoch 12/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.3475e-06 - val_loss: 3.2893e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.0773e-06 - val_loss: 3.7887e-06\n",
      "Epoch 14/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.7506e-06 - val_loss: 1.3471e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.8169e-06 - val_loss: 1.2975e-05\n",
      "Epoch 16/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.1134e-06 - val_loss: 3.6057e-06\n",
      "Epoch 17/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9512e-06 - val_loss: 1.5196e-05\n",
      "Epoch 18/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.3928e-06 - val_loss: 1.8653e-05\n",
      "Epoch 19/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4412e-06 - val_loss: 3.5597e-05\n",
      "Epoch 20/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.1256e-06 - val_loss: 3.2729e-06\n",
      "Epoch 21/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9818e-06 - val_loss: 3.1192e-05\n",
      "Epoch 22/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.6456e-06 - val_loss: 1.2262e-05\n",
      "Epoch 23/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.3493e-06 - val_loss: 1.9655e-05\n",
      "Epoch 24/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.8479e-06 - val_loss: 2.8825e-06\n",
      "Epoch 25/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6927e-06 - val_loss: 3.9902e-06\n",
      "Epoch 26/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.4838e-06 - val_loss: 5.9245e-06\n",
      "Epoch 27/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.5386e-06 - val_loss: 3.1276e-06\n",
      "Epoch 28/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.5987e-06 - val_loss: 7.3494e-06\n",
      "Epoch 29/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.3936e-06 - val_loss: 2.3873e-06\n",
      "Epoch 30/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.7367e-06 - val_loss: 3.7337e-06\n",
      "Epoch 31/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4819e-06 - val_loss: 6.9851e-06\n",
      "Epoch 32/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.1960e-06 - val_loss: 1.1585e-05\n",
      "Epoch 33/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2679e-06 - val_loss: 3.3346e-06\n",
      "Epoch 34/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9717e-06 - val_loss: 1.0403e-05\n",
      "Epoch 35/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.7160e-06 - val_loss: 1.1758e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.9200e-06 - val_loss: 5.3623e-06\n",
      "Epoch 37/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.5095e-06 - val_loss: 2.4777e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.5799e-06 - val_loss: 2.2908e-06\n",
      "Epoch 39/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.0102e-06 - val_loss: 7.3208e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.8097e-06 - val_loss: 6.0307e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.7561e-06 - val_loss: 5.4861e-06\n",
      "Epoch 42/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3603e-06 - val_loss: 2.9023e-06\n",
      "Epoch 43/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.8794e-06 - val_loss: 6.9803e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.4646e-06 - val_loss: 7.9277e-06\n",
      "Epoch 45/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.8513e-06 - val_loss: 6.3703e-06\n",
      "Epoch 46/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6787e-06 - val_loss: 1.2357e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1503e-06 - val_loss: 4.6527e-06\n",
      "Epoch 48/200\n",
      "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.7888e-06 - val_loss: 2.4588e-06\n",
      "\u001b[1m1387/1387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "regressor1 = RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "regressor1.fit(X_train,y_train.values.ravel())\n",
    "rfr_pred= regressor1.predict(X_test)\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "polyregression = PolynomialFeatures(degree=1)\n",
    "X_poly = polyregression.fit_transform(X_train)\n",
    "polyregression.fit(X_poly, y_train)\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_poly, y_train)\n",
    "poly_pred= linear.predict(polyregression.fit_transform(X_test))\n",
    "\n",
    "# LSTM\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(25, activation='tanh', recurrent_activation='sigmoid', return_sequences=True,input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(25, return_sequences=True))\n",
    "lstm_model.add(LSTM(25, return_sequences=True))\n",
    "lstm_model.add(LSTM(25))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=200,batch_size=32, verbose=1,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# LSTM Predictions\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "lstm_pred = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Calculate performance weighted average RFR+lstm\n",
    "rmse_rfr = np.sqrt(metrics.mean_squared_error(y_test, rfr_pred))\n",
    "rmse_lstm = np.sqrt(metrics.mean_squared_error(y_test, lstm_pred))\n",
    "\n",
    "# Assign weights inversely proportional to MSE\n",
    "weight_rfr = 1 / rmse_rfr\n",
    "weight_lstm = 1 / rmse_lstm\n",
    "total_weight = weight_rfr + weight_lstm\n",
    "weight_rfr /= total_weight\n",
    "weight_lstm /= total_weight\n",
    "\n",
    "rfr_pred = rfr_pred.reshape(len(rfr_pred),1)\n",
    "rfr_lstm = (weight_rfr * rfr_pred + weight_lstm * lstm_pred)\n",
    "\n",
    "# Calculate performance weighted average poly+lstm\n",
    "\n",
    "rmse_poly = np.sqrt(metrics.mean_squared_error(y_test, poly_pred))\n",
    "rmse_lstm = np.sqrt(metrics.mean_squared_error(y_test, lstm_pred))\n",
    "\n",
    "# Assign weights inversely proportional to MSE\n",
    "\n",
    "weight_poly = 1 / rmse_poly\n",
    "weight_lstm = 1 / rmse_lstm\n",
    "total_weight = weight_poly + weight_lstm\n",
    "weight_poly /= total_weight\n",
    "weight_lstm /= total_weight\n",
    "\n",
    "# Combine predictions\n",
    "poly_lstm = (weight_poly * poly_pred + weight_lstm * lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Graph\n",
    "# run in google collab requires high ram capacity due to the mplot3d graph\n",
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure(figsize=(16, 16))#, dpi=150)\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "#plot data\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), np.zeros_like(X_test[\"cycle\"]), np.array(y_test), color='blue', label='Actual')\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), np.ones_like(X_test[\"cycle\"]), np.array(poly_lstm), color='green', label=\"Poly+Lstm\")\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), 2*np.ones_like(X_test[\"cycle\"]), np.array(rfr_lstm), color='violet', label=\"Rfr+Lstm\")\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), 3*np.ones_like(X_test[\"cycle\"]), np.array(rfr_pred), color='orange', label=\"Rfr\")\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), 4*np.ones_like(X_test[\"cycle\"]), np.array(poly_pred), color='purple', label=\"Poly\")\n",
    "ax.plot(np.array(X_test[[\"cycle\"]]), 5*np.ones_like(X_test[\"cycle\"]), np.array(lstm_pred), color='red', label=\"lstm\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# graph properties\n",
    "\n",
    "ax.set_xlabel(\"Cycle\")\n",
    "ax.set_ylabel(\"    Method\")\n",
    "ax.set_zlabel(\"SOH\")\n",
    "ax.set_yticks([0, 1, 2, 3, 4, 5])\n",
    "ax.set_yticklabels(['actual', 'Poly+Lstm', 'Rfr+lstm', 'Rfr', 'Poly','Lstm'])\n",
    "ax.set_title(\"Comparison Graph\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
